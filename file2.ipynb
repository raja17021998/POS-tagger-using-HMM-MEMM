{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AT', 'AT', 'AT', 'AT', 'AT']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast  # For converting string representations of lists to actual lists\n",
    "\n",
    "def parse_tagged_sentences(df):\n",
    "    # Convert string representations of lists to actual lists\n",
    "    df['tagged_sentence'] = df['tagged_sentence'].apply(ast.literal_eval)\n",
    "    \n",
    "    all_words = []\n",
    "    all_tags = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentence_words, sentence_tags = zip(*row['tagged_sentence'])  # Unzipping words and tags\n",
    "        all_words.extend(sentence_words)\n",
    "        all_tags.extend(sentence_tags)\n",
    "    return all_words, all_tags\n",
    "\n",
    "def train_hmm(words, tags):\n",
    "    unique_words = list(set(words))\n",
    "    unique_tags = list(set(tags))\n",
    "    \n",
    "    # Initialize counts\n",
    "    tag_counts = {tag: 0 for tag in unique_tags}\n",
    "    transition_counts = {tag: {t: 0 for t in unique_tags} for tag in unique_tags}\n",
    "    emission_counts = {tag: {word: 0 for word in unique_words} for tag in unique_tags}\n",
    "    \n",
    "    # Count initial tag frequencies\n",
    "    initial_tag_counts = {tag: 0 for tag in unique_tags}\n",
    "    initial_tag_counts[tags[0]] += 1  # Assuming the first tag of the list is the start of a sentence\n",
    "    \n",
    "    # Update counts based on the dataset\n",
    "    for i in range(len(tags) - 1):\n",
    "        current_tag, next_tag = tags[i], tags[i + 1]\n",
    "        word = words[i]\n",
    "        tag_counts[current_tag] += 1\n",
    "        transition_counts[current_tag][next_tag] += 1\n",
    "        emission_counts[current_tag][word] += 1\n",
    "        \n",
    "    # Convert counts to probabilities\n",
    "    transition_prob = {\n",
    "        current_tag: {next_tag: transition_counts[current_tag][next_tag] / tag_counts[current_tag]\n",
    "                      for next_tag in unique_tags} for current_tag in unique_tags\n",
    "    }\n",
    "    \n",
    "    emission_prob = {\n",
    "        tag: {word: (emission_counts[tag][word] / tag_counts[tag] if tag_counts[tag] > 0 else 0)\n",
    "              for word in unique_words} for tag in unique_tags\n",
    "    }\n",
    "    \n",
    "    initial_prob = {tag: initial_tag_counts[tag] / sum(initial_tag_counts.values()) for tag in unique_tags}\n",
    "    \n",
    "    return unique_tags, unique_words, transition_prob, emission_prob, initial_prob\n",
    "\n",
    "def viterbi_algorithm(sentence, unique_tags, transition_prob, emission_prob, initial_prob):\n",
    "    states = np.zeros((len(unique_tags), len(sentence)))\n",
    "    backpointer = np.zeros((len(unique_tags), len(sentence)), dtype=int)\n",
    "    \n",
    "    # Initialize with initial probabilities\n",
    "    for i, tag in enumerate(unique_tags):\n",
    "        states[i, 0] = initial_prob[tag] * emission_prob[tag].get(sentence[0], 0)\n",
    "    \n",
    "    # Populate the rest of the states\n",
    "    for t in range(1, len(sentence)):\n",
    "        for i, tag in enumerate(unique_tags):\n",
    "            max_prob = max(states[j, t-1] * transition_prob[unique_tags[j]][tag] * emission_prob[tag].get(sentence[t], 0) \n",
    "                           for j in range(len(unique_tags)))\n",
    "            states[i, t] = max_prob\n",
    "            backpointer[i, t] = np.argmax([states[j, t-1] * transition_prob[unique_tags[j]][tag] for j in range(len(unique_tags))])\n",
    "    \n",
    "    # Backtracking\n",
    "    best_path = [np.argmax(states[:, len(sentence) - 1])]\n",
    "    for t in range(len(sentence) - 1, 0, -1):\n",
    "        best_path.insert(0, backpointer[best_path[0], t])\n",
    "    \n",
    "    return [unique_tags[index] for index in best_path]\n",
    "\n",
    "def main():\n",
    "    train_filepath = 'train.csv'\n",
    "    df = pd.read_csv(train_filepath)\n",
    "    words, tags = parse_tagged_sentences(df)\n",
    "    unique_tags, unique_words, transition_prob, emission_prob, initial_prob = train_hmm(words, tags)\n",
    "    \n",
    "    # Example: Predict the POS tags for a new sentence\n",
    "    # You will need to preprocess the sentence similar to your training data before prediction\n",
    "    sentence = ['Mary', 'is', 'an', 'example', 'sentence']\n",
    "    predicted_tags = viterbi_algorithm(sentence, unique_tags, transition_prob, emission_prob, initial_prob)\n",
    "    \n",
    "    print(predicted_tags)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
